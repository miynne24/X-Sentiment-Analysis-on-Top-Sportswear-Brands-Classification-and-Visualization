{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a857863-0034-486e-aa03-fd6583779de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48fb7e57-45bf-420f-ac51-877179b1a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "em=pd.read_csv('MMT_donepreprocess.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b6a3cd0-bc7d-470b-813d-948fdad8553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f116e70-06c2-4b0a-ae0a-7efa43c411e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Indonesian stop words\n",
    "indonesian_malay_stop_words = [\n",
    "    'yang', 'dan', 'di', 'dari', 'ke', 'untuk', 'dengan', 'pada', 'adalah', 'oleh',\n",
    "    'sebagai', 'dalam', 'itu', 'ini', 'untuk', 'sebuah', 'juga', 'tidak', 'atau',\n",
    "    'mereka', 'kami', 'saya', 'kamu', 'dia', 'apa', 'satu', 'dua', 'tiga', 'empat', \n",
    "    'lima', 'enam', 'tujuh', 'delapan', 'sembilan', 'sepuluh','yang', 'dan', 'di', 'dari', 'ke', 'untuk', 'dengan', 'pada', 'adalah', 'oleh',\n",
    "    'sebagai', 'dalam', 'itu', 'ini', 'untuk', 'sebuah', 'juga', 'tidak', 'atau',\n",
    "    'mereka', 'kami', 'saya', 'anda', 'dia', 'apa', 'satu', 'dua', 'tiga', 'empat', \n",
    "    'lima', 'enam', 'tujuh', 'lapan', 'sembilan', 'sepuluh','ni','mcm','mahu','yg','pru','spr',\n",
    "    'tu','la','dah','nak','dgn','ok','kau'\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9519f917-8380-4407-b34a-16810ead98a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(max_df=0.95,min_df=2,stop_words=indonesian_malay_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4c028d8d-291f-4cc7-8c2c-1e264447c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm=tfidf.fit_transform(em['normalized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "93acc86b-8019-413b-b499-7a6aad230520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6671x4408 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 52131 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7331aff4-fc6a-40a0-a76c-7572d310997f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['beliau', 'wang', 'perdana', 'bangun', 'ambil', 'isu', 'laku', 'selamat', 'tindak', 'menteri', 'ekonomi', 'politik', 'rana', 'malaysia', 'negara']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['laksana', 'pn', 'pusat', 'je', 'kes', 'rm', 'covid', 'lantan', 'salah', 'gagal', 'sabah', 'bantu', 'selangor', 'negeri', 'raja']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['bodoh', 'asli', 'tua', 'umno', 'muda', 'batu', 'kena', 'je', 'cakap', 'kerja', 'kat', 'politik', 'melayu', 'ramai', 'orang']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['politik', 'bebas', 'lepas', 'pimpin', 'pas', 'harap', 'umno', 'menang', 'tanding', 'sokong', 'calon', 'parti', 'undi', 'raya', 'pilih']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['percaya', 'sahaja', 'beban', 'umno', 'tipu', 'bn', 'bangkang', 'harap', 'baik', 'wakil', 'khidmat', 'malaysia', 'pimpin', 'bantu', 'rakyat']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nmf_model=NMF(n_components=5,random_state=100)\n",
    "nmf_model.fit(dtm)\n",
    "for index, topic in enumerate(nmf_model.components_):\n",
    "    print(f\"THE TOP 15 WORDS FOR TOPIC #{index}\")\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    print([feature_names[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85315a5b-9f80-43dd-ad75-a3ff401f0d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, ..., 0, 0, 3], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results=nmf_model.transform(dtm)\n",
    "topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12291732-48b0-4e58-96b4-c47fe91c87ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "em['Topic']=topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06fc8643-a410-4202-a808-c1ec9fcacb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytopic_dict={\n",
    "    0:'Size Availability',\n",
    "    1:'Comfortability',\n",
    "    2:'Durability',\n",
    "    3:'Discount and Promotion',\n",
    "    4:'Price'}\n",
    "\n",
    "em['Topic Label']=em['Topic'].map(mytopic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8203bcb0-ad3d-4f82-8db5-d3c966aaaef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['beliau', 'wang', 'perdana', 'bangun', 'ambil', 'isu', 'laku', 'selamat', 'tindak', 'menteri', 'ekonomi', 'politik', 'rana', 'malaysia', 'negara']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['laksana', 'pn', 'pusat', 'je', 'kes', 'rm', 'covid', 'lantan', 'salah', 'gagal', 'sabah', 'bantu', 'selangor', 'negeri', 'raja']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['bodoh', 'asli', 'tua', 'umno', 'muda', 'batu', 'kena', 'je', 'cakap', 'kerja', 'kat', 'politik', 'melayu', 'ramai', 'orang']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['politik', 'bebas', 'lepas', 'pimpin', 'pas', 'harap', 'umno', 'menang', 'tanding', 'sokong', 'calon', 'parti', 'undi', 'raya', 'pilih']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['percaya', 'sahaja', 'beban', 'umno', 'tipu', 'bn', 'bangkang', 'harap', 'baik', 'wakil', 'khidmat', 'malaysia', 'pimpin', 'bantu', 'rakyat']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_13336\\2198270135.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'beliau, wang, perdana, bangun, ambil, isu, laku, selamat, tindak, menteri, ekonomi, politik, rana, malaysia, negara' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  em.loc[em['Topic'] == index, 'top_keywords'] = ', '.join(top_keywords)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>case_folding</th>\n",
       "      <th>tokenize</th>\n",
       "      <th>StopWord Removal</th>\n",
       "      <th>stemming_data</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Topic Label</th>\n",
       "      <th>top_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>perempuan tu apesal aku rasa mcm pernah nampak...</td>\n",
       "      <td>0</td>\n",
       "      <td>perempuan tu apesal aku rasa mcm pernah nampak...</td>\n",
       "      <td>perempuan tu apesal aku rasa mcm pernah nampak...</td>\n",
       "      <td>['perempuan', 'tu', 'apesal', 'aku', 'rasa', '...</td>\n",
       "      <td>['perempuan', 'tu', 'apesal', 'mcm', 'nampak',...</td>\n",
       "      <td>perempuan tu apesal mcm nampak student unisel ...</td>\n",
       "      <td>perempuan tu apesal mcm nampak student unisel ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Durability</td>\n",
       "      <td>bodoh, asli, tua, umno, muda, batu, kena, je, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Polis tangkap dia.</td>\n",
       "      <td>-1</td>\n",
       "      <td>Polis tangkap dia</td>\n",
       "      <td>polis tangkap dia</td>\n",
       "      <td>['polis', 'tangkap', 'dia']</td>\n",
       "      <td>['polis', 'tangkap']</td>\n",
       "      <td>polis tangkap</td>\n",
       "      <td>polis tangkap</td>\n",
       "      <td>0</td>\n",
       "      <td>Size Availability</td>\n",
       "      <td>beliau, wang, perdana, bangun, ambil, isu, lak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kenapa lokasi kebakaran nya terlalu spesifik? ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Kenapa lokasi kebakaran nya terlalu spesifik</td>\n",
       "      <td>kenapa lokasi kebakaran nya terlalu spesifik</td>\n",
       "      <td>['kenapa', 'lokasi', 'kebakaran', 'nya', 'terl...</td>\n",
       "      <td>['lokasi', 'kebakaran', 'nya', 'spesifik']</td>\n",
       "      <td>lokasi bakar nya spesifik</td>\n",
       "      <td>lokasi bakar nya spesifik</td>\n",
       "      <td>0</td>\n",
       "      <td>Size Availability</td>\n",
       "      <td>beliau, wang, perdana, bangun, ambil, isu, lak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@YuleumSummer Waa senang nye lah nk bersalin.....</td>\n",
       "      <td>1</td>\n",
       "      <td>Waa senang nye lah nk bersalinape tips alia e...</td>\n",
       "      <td>waa senang nye lah nk bersalinape tips alia e...</td>\n",
       "      <td>['waa', 'senang', 'nye', 'lah', 'nk', 'bersali...</td>\n",
       "      <td>['waa', 'senang', 'nye', 'nk', 'bersalinape', ...</td>\n",
       "      <td>waa senang nye nk bersalinape tips alia ehshar...</td>\n",
       "      <td>waa senang nye nk bersalinape tips alia ehshar...</td>\n",
       "      <td>2</td>\n",
       "      <td>Durability</td>\n",
       "      <td>bodoh, asli, tua, umno, muda, batu, kena, je, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DAULAT TUANKU! Merafak sembah dan takzim KDYMM...</td>\n",
       "      <td>1</td>\n",
       "      <td>DAULAT TUANKU Merafak sembah dan takzim KDYMM ...</td>\n",
       "      <td>daulat tuanku merafak sembah dan takzim kdymm ...</td>\n",
       "      <td>['daulat', 'tuanku', 'merafak', 'sembah', 'dan...</td>\n",
       "      <td>['daulat', 'tuanku', 'merafak', 'sembah', 'tak...</td>\n",
       "      <td>daulat tuanku rafak sembah takzim kdymm seri p...</td>\n",
       "      <td>daulat tuanku rafak sembah takzim kdymm seri p...</td>\n",
       "      <td>0</td>\n",
       "      <td>Size Availability</td>\n",
       "      <td>beliau, wang, perdana, bangun, ambil, isu, lak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0  perempuan tu apesal aku rasa mcm pernah nampak...          0   \n",
       "1                                 Polis tangkap dia.         -1   \n",
       "2  Kenapa lokasi kebakaran nya terlalu spesifik? ...         -1   \n",
       "3  @YuleumSummer Waa senang nye lah nk bersalin.....          1   \n",
       "4  DAULAT TUANKU! Merafak sembah dan takzim KDYMM...          1   \n",
       "\n",
       "                                            cleaning  \\\n",
       "0  perempuan tu apesal aku rasa mcm pernah nampak...   \n",
       "1                                  Polis tangkap dia   \n",
       "2      Kenapa lokasi kebakaran nya terlalu spesifik    \n",
       "3   Waa senang nye lah nk bersalinape tips alia e...   \n",
       "4  DAULAT TUANKU Merafak sembah dan takzim KDYMM ...   \n",
       "\n",
       "                                        case_folding  \\\n",
       "0  perempuan tu apesal aku rasa mcm pernah nampak...   \n",
       "1                                  polis tangkap dia   \n",
       "2      kenapa lokasi kebakaran nya terlalu spesifik    \n",
       "3   waa senang nye lah nk bersalinape tips alia e...   \n",
       "4  daulat tuanku merafak sembah dan takzim kdymm ...   \n",
       "\n",
       "                                            tokenize  \\\n",
       "0  ['perempuan', 'tu', 'apesal', 'aku', 'rasa', '...   \n",
       "1                        ['polis', 'tangkap', 'dia']   \n",
       "2  ['kenapa', 'lokasi', 'kebakaran', 'nya', 'terl...   \n",
       "3  ['waa', 'senang', 'nye', 'lah', 'nk', 'bersali...   \n",
       "4  ['daulat', 'tuanku', 'merafak', 'sembah', 'dan...   \n",
       "\n",
       "                                    StopWord Removal  \\\n",
       "0  ['perempuan', 'tu', 'apesal', 'mcm', 'nampak',...   \n",
       "1                               ['polis', 'tangkap']   \n",
       "2         ['lokasi', 'kebakaran', 'nya', 'spesifik']   \n",
       "3  ['waa', 'senang', 'nye', 'nk', 'bersalinape', ...   \n",
       "4  ['daulat', 'tuanku', 'merafak', 'sembah', 'tak...   \n",
       "\n",
       "                                       stemming_data  \\\n",
       "0  perempuan tu apesal mcm nampak student unisel ...   \n",
       "1                                      polis tangkap   \n",
       "2                          lokasi bakar nya spesifik   \n",
       "3  waa senang nye nk bersalinape tips alia ehshar...   \n",
       "4  daulat tuanku rafak sembah takzim kdymm seri p...   \n",
       "\n",
       "                                     normalized_text  Topic  \\\n",
       "0  perempuan tu apesal mcm nampak student unisel ...      2   \n",
       "1                                      polis tangkap      0   \n",
       "2                          lokasi bakar nya spesifik      0   \n",
       "3  waa senang nye nk bersalinape tips alia ehshar...      2   \n",
       "4  daulat tuanku rafak sembah takzim kdymm seri p...      0   \n",
       "\n",
       "         Topic Label                                       top_keywords  \n",
       "0         Durability  bodoh, asli, tua, umno, muda, batu, kena, je, ...  \n",
       "1  Size Availability  beliau, wang, perdana, bangun, ambil, isu, lak...  \n",
       "2  Size Availability  beliau, wang, perdana, bangun, ambil, isu, lak...  \n",
       "3         Durability  bodoh, asli, tua, umno, muda, batu, kena, je, ...  \n",
       "4  Size Availability  beliau, wang, perdana, bangun, ambil, isu, lak...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, topic in enumerate(nmf_model.components_):\n",
    "    print(f\"THE TOP 15 WORDS FOR TOPIC #{index}\")\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    top_keywords = [feature_names[i] for i in topic.argsort()[-15:]]\n",
    "    print(top_keywords)\n",
    "    print('\\n')\n",
    "    em.loc[em['Topic'] == index, 'top_keywords'] = ', '.join(top_keywords)\n",
    "\n",
    "# Display modified DataFrame\n",
    "em.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b091d39-d520-4e55-b026-676423b1cfa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Topic Label</th>\n",
       "      <th>top_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Durability</td>\n",
       "      <td>bodoh, asli, tua, umno, muda, batu, kena, je, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Size Availability</td>\n",
       "      <td>beliau, wang, perdana, bangun, ambil, isu, lak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Size Availability</td>\n",
       "      <td>beliau, wang, perdana, bangun, ambil, isu, lak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Durability</td>\n",
       "      <td>bodoh, asli, tua, umno, muda, batu, kena, je, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Size Availability</td>\n",
       "      <td>beliau, wang, perdana, bangun, ambil, isu, lak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic        Topic Label                                       top_keywords\n",
       "0      2         Durability  bodoh, asli, tua, umno, muda, batu, kena, je, ...\n",
       "1      0  Size Availability  beliau, wang, perdana, bangun, ambil, isu, lak...\n",
       "2      0  Size Availability  beliau, wang, perdana, bangun, ambil, isu, lak...\n",
       "3      2         Durability  bodoh, asli, tua, umno, muda, batu, kena, je, ...\n",
       "4      0  Size Availability  beliau, wang, perdana, bangun, ambil, isu, lak..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA PREPROCESSING\n",
    "df= pd.DataFrame(em[['Topic','Topic Label','top_keywords']])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a6e03b79-2401-420d-a783-55e40d951fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coherence Score Calculation\n",
    "# Prepare data for gensim\n",
    "texts = [text.split() for text in em['normalized_text']]\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Create NMF topics\n",
    "topics = [[feature_names[i] for i in topic.argsort()[-15:]] for topic in nmf_model.components_]\n",
    "coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa28b67b-60d9-4c2b-987f-e7708bfe49e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohesion Score Calculation\n",
    "def calculate_cohesion(dtm, topic_results):\n",
    "    n_topics = topic_results.shape[1]\n",
    "    cohesion_scores = []\n",
    "    for topic in range(n_topics):\n",
    "        topic_docs_indices = np.where(topic_results.argmax(axis=1) == topic)[0]\n",
    "        if len(topic_docs_indices) > 1:\n",
    "            topic_docs = dtm[topic_docs_indices].toarray()\n",
    "            similarity_matrix = cosine_similarity(topic_docs)\n",
    "            np.fill_diagonal(similarity_matrix, 0)\n",
    "            cohesion = np.mean(similarity_matrix)\n",
    "            cohesion_scores.append(cohesion)\n",
    "        else:\n",
    "            cohesion_scores.append(0)  # Assign zero if only one document or none in the topic\n",
    "    return np.mean(cohesion_scores)\n",
    "\n",
    "cohesion_score = calculate_cohesion(dtm, topic_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "168c4d45-50cc-4b4b-be18-11901bee26bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.4066919762846699\n",
      "Cohesion Score: 0.01865359703994563\n"
     ]
    }
   ],
   "source": [
    "print(f\"Coherence Score: {coherence_score}\")\n",
    "print(f\"Cohesion Score: {cohesion_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ecc5b-9594-49d6-8990-bebc710a59e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
